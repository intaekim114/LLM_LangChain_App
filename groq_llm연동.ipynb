{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95060012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1fa19dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e5e91b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"LangServe는 무엇인가요? 자세하게 설명해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "846007cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001F78263A360> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F7827BD2B0> root_client=<openai.OpenAI object at 0x000001F7FF9D8320> root_async_client=<openai.AsyncOpenAI object at 0x000001F78232B8F0> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa69856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "응답: LangServe는 LangChain이라는 프레임워크를 기반으로 구축된 오픈 소스 라이브러리입니다. LangChain은 자연어 처리(NLP) 및 생성 AI 애플리케이션을 개발하기 위한 프레임워크로, 대규모 언어 모델(LLM)을 활용하여 애플리케이션을 구축할 수 있습니다.\n",
      "\n",
      "LangServe는 LangChain을 사용하여 구축된 애플리케이션을 쉽게 배포하고 관리할 수 있도록 설계되었습니다. 이 라이브러리는 개발자가 LangChain을 사용하여 구축한 애플리케이션을 클라우드 환경에서 쉽게 배포하고, API를 통해 다른 서비스와 통합할 수 있도록 지원합니다.\n",
      "\n",
      "LangServe의 주요 기능은 다음과 같습니다:\n",
      "\n",
      "1. **LangChain 애플리케이션 배포**: LangServe를 사용하면 LangChain을 사용하여 구축된 애플리케이션을 쉽게 배포할 수 있습니다. 개발자는 LangServe를 사용하여 애플리케이션을 클라우드 환경에서 배포하고 관리할 수 있습니다.\n",
      "2. **API 통합**: LangServe는 API를 제공하여 다른 서비스와의 통합을 지원합니다. 이를 통해 개발자는 LangServe를 사용하여 구축된 애플리케이션을 다른 서비스와 쉽게 통합할 수 있습니다.\n",
      "3. **모니터링 및 로그 관리**: LangServe는 애플리케이션의 모니터링 및 로그 관리를 지원합니다. 이를 통해 개발자는 애플리케이션의 성능을 모니터링하고 문제를 빠르게 식별할 수 있습니다.\n",
      "4. **확장성**: LangServe는 수평적 확장성을 지원합니다. 즉, 애플리케이션의 수요가 증가하면 LangServe를 사용하여 쉽게 확장할 수 있습니다.\n",
      "\n",
      "LangServe를 사용하는 이유는 다음과 같습니다:\n",
      "\n",
      "1. **빠른 배포**: LangServe를 사용하면 LangChain 애플리케이션을 빠르게 배포할 수 있습니다.\n",
      "2. **쉬운 관리**: LangServe는 애플리케이션의 관리 및 모니터링을 쉽게 해줍니다.\n",
      "3. **확장성**: LangServe는 애플리케이션의 확장성을 제공합니다.\n",
      "\n",
      "전반적으로, LangServe는 LangChain을 사용하여 구축된 애플리케이션을 쉽게 배포하고 관리할 수 있는 라이브러리입니다. 이를 통해 개발자는 LangChain의 강력한 기능을 활용하여 자연어 처리 및 생성 AI 애플리케이션을 빠르게 구축하고 배포할 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "try:    # llm 호출\n",
    "    response = llm.invoke(prompt_text) \n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03333b28",
   "metadata": {},
   "source": [
    "### LCEL\n",
    "* Prompt + LLM을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93694e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    You are an expert in AI Expert. Answer the question. \\n    <Question>: {input}에 대해 쉽게 한글로 설명해주세요.\\n    ')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in AI Expert. Answer the question. \n",
    "    <Question>: {input}에 대해 쉽게 한글로 설명해주세요.\n",
    "    \"\"\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5a86543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain 연결 (LCEL)\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c723132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "인공지능 모델의 학습 원리는 사람의 뇌가 학습하는 방식과 유사합니다. 컴퓨터가 데이터를 통해 스스로 학습하고, 패턴을 발견하고, 예측이나 분류를 더 잘하도록 만드는 것입니다.\n",
      "\n",
      "**기본 개념:**\n",
      "\n",
      "1. **데이터**: 인공지능 모델이 학습하는 데 필요한 정보입니다. 예를 들어, 이미지, 텍스트, 소리 등이 데이터가 될 수 있습니다.\n",
      "2. **모델**: 데이터를 학습하고, 예측이나 분류를 수행하는 컴퓨터 프로그램입니다.\n",
      "3. **학습**: 모델이 데이터를 통해 스스로를 개선하는 과정입니다.\n",
      "\n",
      "**학습 과정:**\n",
      "\n",
      "1. **데이터 수집**: 다양한 소스에서 데이터를 수집합니다.\n",
      "2. **데이터 전처리**: 수집한 데이터를 깨끗하게 정리하고, 필요한 경우 변환합니다.\n",
      "3. **모델 초기화**: 모델을 처음부터 설정합니다. 이때, 모델의 구조와 파라미터(변수)를 정의합니다.\n",
      "4. **학습 루프**: 모델이 데이터를 학습하는 과정입니다.\n",
      "   - **예측**: 모델이 입력 데이터를 보고, 예측을 합니다.\n",
      "   - **오차 계산**: 예측 결과와 실제 값 사이의 오차를 계산합니다.\n",
      "   - **파라미터 업데이트**: 오차를 최소화하기 위해 모델의 파라미터를 조정합니다.\n",
      "5. **성능 평가**: 학습이 완료되면, 모델의 성능을 평가합니다. 이는 테스트 데이터를 사용하여 모델의 예측 정확도를 확인하는 것입니다.\n",
      "\n",
      "**핵심 아이디어:**\n",
      "\n",
      "- **반복 학습**: 모델은 데이터를 반복적으로 학습하면서 점점 더 나은 성능을 발휘하도록 됩니다.\n",
      "- **오차 최소화**: 모델은 예측 오차를 줄이기 위해 노력합니다. 이를 위해 최적화 알고리즘을 사용합니다.\n",
      "\n",
      "**비유:**\n",
      "\n",
      "인공지능 모델의 학습을 어린아이의 학습 과정에 비유할 수 있습니다. 아이가 사물을 인식하고, 행동하는 법을 배우는 것과 유사합니다.\n",
      "\n",
      "- **경험(데이터)**: 아이가 여러 경험을 통해 학습합니다.\n",
      "- **시도하고 실패하기(예측과 오차 계산)**: 아이가 사물이나 상황을 이해하려고 시도하고, 실패를 통해 더 배웁니다.\n",
      "- **개선**: 반복된 경험과 시도를 통해 아이는 점점 더 나은 이해와 행동을 보입니다.\n",
      "\n",
      "이처럼 인공지능 모델도 데이터를 통해 반복적으로 학습하며, 스스로를 개선해 나갑니다.\n"
     ]
    }
   ],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain.invoke({\"input\": \"인공지능 모델의 학습 원리\"})\n",
    "    print(type(result))\n",
    "    print(result.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
