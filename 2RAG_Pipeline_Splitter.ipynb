{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4433ea41",
   "metadata": {},
   "source": [
    "### 1. CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5397168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# ===================================\n",
    "# 예제 텍스트\n",
    "# ===================================\n",
    "text = \"\"\"RAG는 검색 기반의 텍스트 생성 모델입니다. 기존 언어 모델의 단점을 보완하고, 최신 정보를 제공합니다.\n",
    "특히, 최신 데이터를 반영하는 데 강력한 기능을 제공합니다. \n",
    "RAG는 검색과 생성 단계를 포함합니다. 먼저 관련 문서를 검색하고, 그 다음 검색된 문서를 바탕으로 답변을 생성합니다.\n",
    "이 방식은 환상(hallucination) 문제를 크게 줄여줍니다. 또한 실시간으로 최신 정보를 활용할 수 있어 매우 유용합니다.\"\"\"\n",
    "\n",
    "print(\" 원본 텍스트:\")\n",
    "print(\"-\" * 50)\n",
    "print(text)\n",
    "print(f\"\\n 원본 길이: {len(text)}자\")\n",
    "\n",
    "# ===================================\n",
    "#  다양한 분할 방식 비교\n",
    "# ===================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" 다양한 CharacterTextSplitter 설정 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 기본 설정 (마침표 기준)\n",
    "print(\"\\n 마침표(.) 기준 분할:\")\n",
    "print(\"-\" * 30)\n",
    "splitter1 = CharacterTextSplitter(\n",
    "    chunk_size=50,      # 청크 최대 크기\n",
    "    chunk_overlap=10,   # 청크 간 중복\n",
    "    separator=\".\"       # 분할 기준\n",
    ")\n",
    "chunks1 = splitter1.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks1, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}' (길이: {len(chunk)}자)\")\n",
    "\n",
    "#  문장 기준 (좀 더 큰 청크)\n",
    "print(\"\\n 문장 기준 분할 (큰 청크):\")\n",
    "print(\"-\" * 30)\n",
    "splitter2 = CharacterTextSplitter(\n",
    "    chunk_size=100,     # 더 큰 청크\n",
    "    chunk_overlap=20,   # 더 많은 중복\n",
    "    separator=\".\"\n",
    ")\n",
    "chunks2 = splitter2.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks2, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}' (길이: {len(chunk)}자)\")\n",
    "\n",
    "#  줄바꿈 기준\n",
    "print(\"\\n 줄바꿈(\\\\n) 기준 분할:\")\n",
    "print(\"-\" * 30)\n",
    "splitter3 = CharacterTextSplitter(\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=0,    # 중복 없음\n",
    "    separator=\"\\n\"\n",
    ")\n",
    "chunks3 = splitter3.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks3, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}' (길이: {len(chunk)}자)\")\n",
    "\n",
    "#  공백 기준 (단어 단위)\n",
    "print(\"\\n 공백(' ') 기준 분할 (단어 단위):\")\n",
    "print(\"-\" * 30)\n",
    "splitter4 = CharacterTextSplitter(\n",
    "    chunk_size=30,      # 작은 청크\n",
    "    chunk_overlap=5,\n",
    "    separator=\" \"       # 공백으로 분할\n",
    ")\n",
    "chunks4 = splitter4.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(chunks4[:5], 1):  # 처음 5개만 출력\n",
    "    print(f\"청크 {i}: '{chunk.strip()}' (길이: {len(chunk)}자)\")\n",
    "print(f\"... 총 {len(chunks4)}개 청크 생성됨\")\n",
    "\n",
    "# ===================================\n",
    "#  설정별 결과 비교\n",
    "# ===================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" 설정별 결과 요약\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = [\n",
    "    (\"마침표 기준 (50자)\", len(chunks1), chunks1),\n",
    "    (\"마침표 기준 (100자)\", len(chunks2), chunks2),\n",
    "    (\"줄바꿈 기준\", len(chunks3), chunks3),\n",
    "    (\"공백 기준\", len(chunks4), chunks4)\n",
    "]\n",
    "\n",
    "for name, count, chunks in results:\n",
    "    avg_length = sum(len(chunk) for chunk in chunks) / len(chunks)\n",
    "    print(f\"{name:15}: {count:2}개 청크, 평균 {avg_length:.1f}자\")\n",
    "\n",
    "# ===================================\n",
    "#  chunk_overlap 효과 확인\n",
    "# ===================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" chunk_overlap 효과 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 중복 없음\n",
    "splitter_no_overlap = CharacterTextSplitter(\n",
    "    chunk_size=50, chunk_overlap=0, separator=\".\"\n",
    ")\n",
    "chunks_no_overlap = splitter_no_overlap.split_text(text)\n",
    "\n",
    "# 중복 있음\n",
    "splitter_with_overlap = CharacterTextSplitter(\n",
    "    chunk_size=50, chunk_overlap=15, separator=\".\"\n",
    ")\n",
    "chunks_with_overlap = splitter_with_overlap.split_text(text)\n",
    "\n",
    "print(\"\\n 중복 없음 (overlap=0):\")\n",
    "for i, chunk in enumerate(chunks_no_overlap, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}'\")\n",
    "\n",
    "print(\"\\n 중복 있음 (overlap=15):\")\n",
    "for i, chunk in enumerate(chunks_with_overlap, 1):\n",
    "    print(f\"청크 {i}: '{chunk.strip()}'\")\n",
    "    if i > 1:  # 두 번째 청크부터 중복 부분 표시\n",
    "        prev_chunk = chunks_with_overlap[i-2].strip()\n",
    "        curr_chunk = chunk.strip()\n",
    "        # 간단한 중복 확인\n",
    "        if len(prev_chunk) > 10 and len(curr_chunk) > 10:\n",
    "            if prev_chunk[-10:] in curr_chunk:\n",
    "                print(f\"    이전 청크와 중복: '{prev_chunk[-10:]}'\")\n",
    "\n",
    "# ===================================\n",
    "#  실무 팁\n",
    "# ===================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" 실무 활용 팁\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "tips = \"\"\"\n",
    " 청크 크기 가이드:\n",
    "   • 짧은 문서: 200-500자\n",
    "   • 긴 문서: 500-1000자\n",
    "   • 매우 긴 문서: 1000-2000자\n",
    "\n",
    " chunk_overlap 가이드:\n",
    "   • 일반적: 청크 크기의 10-20%\n",
    "   • 맥락 중요: 청크 크기의 20-30%\n",
    "   • 속도 중요: 0-10%\n",
    "\n",
    " separator 선택:\n",
    "   • 문서: 문단(\\n\\n) 또는 문장(.)\n",
    "   • 코드: 함수나 클래스 단위\n",
    "   • 대화: 발화자 변경 지점\n",
    "\n",
    " RAG 최적화:\n",
    "   • 너무 작으면 맥락 손실\n",
    "   • 너무 크면 관련성 저하\n",
    "   • 적절한 중복으로 연결성 유지\n",
    "\"\"\"\n",
    "\n",
    "print(tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed16b53",
   "metadata": {},
   "source": [
    "### 2. RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbfb40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 예제 텍스트\n",
    "text = \"\"\"RAG는 검색과 생성 단계를 포함하는 모델입니다.\n",
    "\n",
    "이 모델은 검색 기반의 텍스트 생성 기능을 제공합니다.\n",
    "특히, 최신 데이터를 반영하는 데 강력한 기능을 가지고 있습니다.\n",
    "\n",
    "Transformer 모델을 기반으로 실시간 정보를 활용할 수 있으며, 기존의 단순한 생성 모델보다 더 정확한 답변을 제공합니다.\n",
    "\n",
    "RAG의 핵심은 검색과 생성의 결합입니다! 먼저 관련 문서를 찾고, 그 정보를 바탕으로 답변을 만듭니다.\"\"\"\n",
    "\n",
    "print(\"원본 텍스트:\")\n",
    "print(\"-\" * 50)\n",
    "print(text)\n",
    "print(f\"\\n텍스트 길이: {len(text)}자\")\n",
    "\n",
    "# ===========================================\n",
    "# Recursive vs Character 비교\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RecursiveCharacterTextSplitter vs CharacterTextSplitter 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. RecursiveCharacterTextSplitter (추천)\n",
    "print(\"\\n1. RecursiveCharacterTextSplitter (계층적 분할):\")\n",
    "print(\"-\" * 45)\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=20,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \" \", \"\"]  # 우선순위 순서\n",
    ")\n",
    "recursive_chunks = recursive_splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(recursive_chunks):\n",
    "    print(f\"Chunk {i+1}: '{chunk.strip()}'\")\n",
    "    print(f\"길이: {len(chunk)}자\")\n",
    "    print()\n",
    "\n",
    "# 2. CharacterTextSplitter (비교용)\n",
    "print(\"2. CharacterTextSplitter (단순 분할):\")\n",
    "print(\"-\" * 35)\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "simple_splitter = CharacterTextSplitter(\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=20,\n",
    "    separator=\".\"  # 하나의 구분자만 사용\n",
    ")\n",
    "simple_chunks = simple_splitter.split_text(text)\n",
    "\n",
    "for i, chunk in enumerate(simple_chunks):\n",
    "    print(f\"Chunk {i+1}: '{chunk.strip()}'\")\n",
    "    print(f\"길이: {len(chunk)}자\")\n",
    "    print()\n",
    "\n",
    "# ===========================================\n",
    "# separators 우선순위 테스트\n",
    "# ===========================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"separators 우선순위 동작 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_text = \"\"\"첫 번째 문단입니다.\n",
    "\n",
    "두 번째 문단입니다.\n",
    "이 문단은 여러 문장으로 구성됩니다! 정말 흥미롭죠?\n",
    "\n",
    "세 번째 문단입니다.\"\"\"\n",
    "\n",
    "print(\"테스트 텍스트:\")\n",
    "print(repr(test_text))  # 줄바꿈 문자까지 보이도록\n",
    "\n",
    "# 다양한 separators 설정 테스트\n",
    "separators_configs = [\n",
    "    ([\"\\n\\n\", \"\\n\", \".\", \" \"], \"문단 우선\"),\n",
    "    ([\"\\n\", \".\", \" \"], \"줄바꿈 우선\"),\n",
    "    ([\".\", \"!\", \"?\", \" \"], \"문장 우선\"),\n",
    "    ([\" \"], \"단어 단위\")\n",
    "]\n",
    "\n",
    "for separators, description in separators_configs:\n",
    "    print(f\"\\n{description} separators={separators}:\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=40,\n",
    "        chunk_overlap=10,\n",
    "        separators=separators\n",
    "    )\n",
    "    chunks = splitter.split_text(test_text)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"  Chunk {i}: '{chunk.strip()}'\")\n",
    "\n",
    "# ===========================================\n",
    "# chunk_size별 결과 비교\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"chunk_size별 분할 결과 비교\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "chunk_sizes = [50, 100, 150]\n",
    "\n",
    "for size in chunk_sizes:\n",
    "    print(f\"\\nchunk_size={size}:\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=20,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    \n",
    "    print(f\"총 {len(chunks)}개 청크 생성\")\n",
    "    avg_length = sum(len(chunk) for chunk in chunks) / len(chunks)\n",
    "    print(f\"평균 청크 길이: {avg_length:.1f}자\")\n",
    "    \n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"  Chunk {i}: '{chunk.strip()[:30]}...' (길이: {len(chunk)}자)\")\n",
    "\n",
    "# ===========================================\n",
    "# chunk_overlap 효과 확인\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"chunk_overlap 효과 확인\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "overlap_values = [0, 10, 30]\n",
    "\n",
    "for overlap in overlap_values:\n",
    "    print(f\"\\nchunk_overlap={overlap}:\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=80,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=[\"\\n\\n\", \".\", \" \"]\n",
    "    )\n",
    "    chunks = splitter.split_text(text)\n",
    "    \n",
    "    print(f\"총 {len(chunks)}개 청크 생성\")\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        print(f\"  Chunk {i}: '{chunk.strip()}'\")\n",
    "        \n",
    "        # 중복 부분 확인\n",
    "        if i > 1 and overlap > 0:\n",
    "            prev_chunk = chunks[i-2].strip()\n",
    "            curr_chunk = chunk.strip()\n",
    "            # 간단한 중복 확인 (마지막 10자와 첫 10자 비교)\n",
    "            if len(prev_chunk) >= 10 and len(curr_chunk) >= 10:\n",
    "                prev_end = prev_chunk[-10:]\n",
    "                curr_start = curr_chunk[:10]\n",
    "                if any(word in curr_start for word in prev_end.split() if len(word) > 2):\n",
    "                    print(f\"    중복 감지: 이전 청크와 겹치는 부분 있음\")\n",
    "\n",
    "# ===========================================\n",
    "# 활용 가이드\n",
    "# ===========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"실무 활용 가이드\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "RecursiveCharacterTextSplitter 사용 가이드:\n",
    "\n",
    "1. 기본 설정 (일반적 문서):\n",
    "   chunk_size=1000, chunk_overlap=200\n",
    "   separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]\n",
    "\n",
    "2. 한국어 문서 최적화:\n",
    "   chunk_size=500-1000, chunk_overlap=100-200\n",
    "   separators=[\"\\n\\n\", \"\\n\", \".\", \"。\", \" \"]\n",
    "\n",
    "3. 코드 문서:\n",
    "   separators=[\"\\n\\n\", \"\\n\", \"\\t\", \" \"]\n",
    "\n",
    "4. 대화/채팅 로그:\n",
    "   separators=[\"\\n\\n\", \"\\n\", \":\", \" \"]\n",
    "\n",
    "장점:\n",
    "- 의미 단위로 자연스러운 분할\n",
    "- 계층적 구분자로 최적화된 분할점 찾기\n",
    "- 텍스트 특성에 맞는 유연한 설정\n",
    "\n",
    "주의사항:\n",
    "- chunk_size는 LLM 토큰 제한 고려\n",
    "- chunk_overlap은 맥락 보존과 비용의 균형\n",
    "- separators 순서가 분할 품질 결정\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n프로그램 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b4a267",
   "metadata": {},
   "source": [
    "### 3. TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850df93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tiktoken\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# 파일 읽기\n",
    "with open(\"./data/ai-terminology.txt\", encoding=\"utf-8\") as f:\n",
    "    file = f.read()  # 파일 내용을 읽어오기\n",
    "\n",
    "print(\"원본 텍스트 미리보기:\\n\", file[:500])  # 앞 500자 출력\n",
    "\n",
    "# TokenTextSplitter 설정\n",
    "text_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=200,  # 청크 크기\n",
    "    chunk_overlap=20,  # 청크 간 겹치는 부분 추가하여 문맥 유지\n",
    "    encoding_name=\"cl100k_base\",  # OpenAI tiktoken 기본 인코딩 사용 (한글 처리 개선)\n",
    "    add_start_index=True  # 각 청크의 시작 인덱스 반환\n",
    ")\n",
    "\n",
    "# 텍스트 분할 실행\n",
    "texts = text_splitter.split_text(file)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"\\n 총 {len(texts)}개의 청크로 분할됨.\")\n",
    "print(\"\\n 첫 번째 청크:\\n\", texts[0])\n",
    "\n",
    "# 청크 길이 확인\n",
    "for i, chunk in enumerate(texts[:5]):  # 처음 5개만 확인\n",
    "    print(f\"\\n Chunk {i+1} (길이: {len(chunk)}):\\n{chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775bfd37",
   "metadata": {},
   "source": [
    "### 4. Hugging Face Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c48be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# GPT-2 모델의 토크나이저 로드\n",
    "hf_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 데이터 파일 읽기\n",
    "file_path = \"./data/ai-terminology.txt\"\n",
    "with open(file_path, encoding=\"utf-8\") as f:\n",
    "    file_content = f.read()\n",
    "\n",
    "print(\" 원본 텍스트 미리보기:\\n\", file_content[:200])\n",
    "\n",
    "# CharacterTextSplitter 설정 (Hugging Face 토크나이저 사용)\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(\n",
    "    hf_tokenizer,\n",
    "    chunk_size=300,  # 각 청크 크기 (토큰 기준 아님)\n",
    "    chunk_overlap=50,  # 청크 간 중복 부분\n",
    ")\n",
    "\n",
    "# 텍스트 분할 수행\n",
    "split_texts = text_splitter.split_text(file_content)\n",
    "\n",
    "# 분할된 텍스트 출력\n",
    "print(f\"\\n 총 {len(split_texts)}개의 청크로 분할됨\\n\")\n",
    "for i, chunk in enumerate(split_texts[:5]):  # 처음 5개만 출력\n",
    "    print(f\" Chunk {i+1} ({len(chunk)}자):\\n{chunk}\\n\")\n",
    "\n",
    "# 토크나이저로 텍스트를 토큰 단위로 변환하여 확인\n",
    "tokenized_example = hf_tokenizer.tokenize(split_texts[0])\n",
    "print(f\"\\n 첫 번째 청크의 토큰 개수: {len(tokenized_example)}\")\n",
    "print(\" 첫 번째 청크의 토큰 리스트:\", tokenized_example[:20])  # 앞 20개만 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
